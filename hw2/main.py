import nltk
#nltk.download('punkt_tab')

text = "Natural Language Processing is a branch of artificial intelligence."

from nltk.tokenize import word_tokenize #tokenize kelimelere ya da cÃ¼mlelere ya da anlamlÄ± kÃ¼Ã§Ã¼k parÃ§alara ayÄ±rÄ±r 

tokens = word_tokenize(text) #kelimelere ayÄ±rÄ±r
print(tokens)
from nltk.corpus import stopwords
#nltk.download('stopwords') #Stopword, bir dilde Ã§ok sÄ±k geÃ§en ama Ã§oÄŸu zaman analizde anlamsal katkÄ±sÄ± az olan kelimelerdir Burada Ã¶nemli kelimeler: yazÄ±, paylaÅŸmak
#Stopword olanlar: bu, seninle, istedim

stop_words = set(stopwords.words('english')) #dosyadaki kelimeleri okur #set->Bir liste verirsen iÃ§indeki tekrar edenleri kaldÄ±rÄ±p kÃ¼me (set) yapÄ±sÄ± oluÅŸturur.
filtered_tokens = [word for word in tokens if word not in stop_words]
""" The following code creates a  filtered list of tokens by removing stop words. parantez iÃ§indekinin uzun hali
filtered = []
 for word in tokens:
    if word not in stop_words:
         filtered.append(word)"""

print(filtered_tokens)

#Lemmatization: kelimelerin temel formunu bulur. kÃ¶k haline getirir.
#running -> run
from nltk.stem import WordNetLemmatizer
nltk.download('wordnet')
lemmatizer = WordNetLemmatizer()
print(lemmatizer.lemmatize("running" , pos="v")) #part of speech v: verb, n: noun, a: adjective, r: adverb o an lemmetize ettiÄŸimiz kelimenin tÃ¼rÃ¼nÃ¼ belirler 

#metin sÄ±nÄ±flandÄ±rma iÅŸlemi iÃ§in arama motorunda Ã§okÃ§a kullanÄ±lÄ±r
#nltk.download('averaged_perceptron_tagger_eng') # average per.tagger->Bu, cÃ¼mledeki her kelimenin isim mi, fiil mi, sÄ±fat mÄ± olduÄŸunu tahmin eden bir makine Ã¶ÄŸrenmesi algoritmasÄ±dÄ±r.
from nltk import pos_tag 
pos_tags= pos_tag(filtered_tokens)
print(pos_tags)
#Ä± am running derken [('I', 'PRP'), ('am', 'VBP'), ('running', 'VBG')]  ÅŸeklinde sÄ±nÄ±flar kelimeleri average perception tagger

#nltk.download('maxent_ne_chunker_tab')
#Bu, NLTK kÃ¼tÃ¼phanesinin iÃ§inde bulunan ve Ã¶zel adlarÄ± (kiÅŸiler, yerler, kuruluÅŸlar gibi) metin iÃ§inde tanÄ±maya yarayan makine Ã¶ÄŸrenmesi tabanlÄ± bir modeldir.
#AÃ§Ä±lÄ±mÄ±:
#maxent = Maximum Entropy (bir istatistiksel model tÃ¼rÃ¼)
#ne = Named Entity (Ã¶zel ad)
#chunker = CÃ¼mle parÃ§alama/etiketleme aracÄ±
#tab = Yeni versiyon, etiketleri bir tablo modeli ile verir (eski chunker'a gÃ¶re daha modern)


#ğŸ“¦ NLPâ€™de MaxEnt Ne Ä°ÅŸe Yarar?
#DoÄŸal dil iÅŸleme problemlerinde ÅŸunu yapar:
#Bir kelimenin ne tÃ¼r bir varlÄ±k (entity) olduÄŸunu tahmin eder:
#PERSON, LOCATION, ORGANIZATION, vs.
#Model, cÃ¼mle iÃ§inde kelimenin kendisine ve baÄŸlamÄ±na bakar ve bir olasÄ±lÄ±k daÄŸÄ±lÄ±mÄ±nÄ± tahmin eder.


nltk.download('words')

from nltk import ne_chunk #named entity recognition
tree = ne_chunk(pos_tags)
print(tree)

#Metin temizleme ve Ã¶n iÅŸleme 
#Lowercading: bÃ¼yÃ¼k harfleri kÃ¼Ã§Ã¼k harfe Ã§evirir.

text = "Natural Language Processing is a branch of artificial intelligence. 100%"
text = text.lower()
print(text)
 #noktalama iÅŸaretlerini kaldÄ±rÄ±r
import re
text = re.sub(r'[^\w\s]', '', text) #\w: kelime karakteri, \s: boÅŸluk karakteri, ^: baÅŸlangÄ±Ã§, $: bitiÅŸ re->regular expression
print(text)
#sayÄ±larÄ± kaldÄ±rÄ±r.
text = re.sub(r'\d+', '', text) #\d: sayÄ± karakteri, +: bir veya daha fazla sayÄ± karakteri
print(text)

#vectorization: metinleri sayÄ±sal deÄŸerlere Ã§evirir.
corpus = ["I love programming", "I love natural language processing", "I love artificial intelligence", "I love python"]
#bag of words modeli vektÃ¶rize modelidir
from sklearn.feature_extraction.text import CountVectorizer
#CountVectorizer, kelimelerin kaÃ§ kez geÃ§tiÄŸini sayan bir vektÃ¶r Ã¼reticidir.
#Metindeki her kelimeyi bir Ã¶zellik (feature) olarak kabul eder.
#Ã–rneÄŸin, "I love programming" metninde "I", "love", "programming" kelimeleri Ã¶zellikler olarak kabul edilir.
#Bu Ã¶zelliklerin sayÄ±sal deÄŸerleri, metinlerin sayÄ±sal temsillerini oluÅŸturur.

vectorizer = CountVectorizer()
#vectorizer adÄ±nda bir nesne oluÅŸturuyorsun.
#HenÃ¼z metne uygulamadÄ±n, sadece aracÄ± tanÄ±mladÄ±n.

X = vectorizer.fit_transform(corpus)
#fit_transform ÅŸunu yapar:
#fit: TÃ¼m kelimeleri Ã¶ÄŸrenir (sÃ¶zlÃ¼k oluÅŸturur).
#transform: Her cÃ¼mleyi bir kelime sayÄ±sÄ± vektÃ¶rÃ¼ne Ã§evirir.
print(vectorizer.get_feature_names_out()) #kelimeleri gÃ¶rÃ¼ntÃ¼ler
print(X.toarray())
# X bir sparse matrix (seyrek matris) nesnesidir.
# toarray() onu tam gÃ¶rÃ¼nÃ¼mlÃ¼ bir 2D NumPy dizisine Ã§evirir.
# Her satÄ±r = bir cÃ¼mle
# Her sÃ¼tun = bir kelime
# Her hÃ¼cre = o kelimenin o cÃ¼mlede kaÃ§ kez geÃ§tiÄŸi
print("--------------------------------------------")
#tf-idf modeli vektÃ¶rize modelidir term frequency-inverse document frequency modelidir
#tf: bir kelimenin bir cÃ¼mlede kaÃ§ kez geÃ§tiÄŸi
#idf: bir kelimenin kaÃ§ cÃ¼mlede geÃ§tiÄŸi
#tf-idf: bir kelimenin bir cÃ¼mlede kaÃ§ kez geÃ§tiÄŸi ve kaÃ§ cÃ¼mlede geÃ§tiÄŸi, nadirliÄŸi
from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer2 = TfidfVectorizer()
X2 = vectorizer2.fit_transform(corpus)
print(vectorizer2.get_feature_names_out()) #kelimeleri gÃ¶rÃ¼ntÃ¼ler
print(X2.toarray())
#deÄŸeri bie yakÄ±nsa o kelime o cÃ¼mlede Ã§ok Ã¶nemli demektir.
#deÄŸeri 0 ise o kelime o cÃ¼mlede hiÃ§ geÃ§memiÅŸ demektir.


#N-gram modeli: bir kelimenin bir cÃ¼mlede kaÃ§ kez geÃ§tiÄŸi
"""| N-Gram TÃ¼rÃ¼ | Ã–rnek             | AmaÃ§                        |
| ----------- | ----------------- | --------------------------- |
| Unigram     | `"love"`          | Kelime frekansÄ±na bakar     |
| Bigram      | `"love Python"`   | BaÄŸlam bilgisi ekler        |
| Trigram     | `"I love Python"` | Daha detaylÄ± baÄŸlam bilgisi |"""


# Otomatik tamamlama, spam tespiti, yazÄ±m Ã¶nerisi.
# Nerede kullanÄ±lÄ±r? => Dilin anlamÄ±nÄ± anlamaz. Sadece istatistiksel olarak kullanÄ±lÄ±r.

# Apple is a fruit.
# Apple is a company.

corpus = [
    "NLP Ã§ok eÄŸlenceli alan",
    "DoÄŸal dil iÅŸleme Ã§ok Ã¶nemli",
    "EÄŸlenceli projeler yapÄ±yoruz"
]

from sklearn.feature_extraction.text import TfidfVectorizer
# Unigram ve Bigram birlikte kullanÄ±lÄ±r (1,2) derken
vectorizer = TfidfVectorizer(ngram_range=(1,2), lowercase=True) 
#tfid kullandÄ±k Ã§Ã¼nkÃ¼ vectÃ¶rize ettik unigram ve bigramÄ± makine sayÄ±larÄ± anlar sadece ondan
X = vectorizer.fit_transform(corpus)

print(f"Feature Names: {vectorizer.get_feature_names_out()}")
print(f"X: {X.toarray()}")



# Word Embedding
# Her kelimeye sayÄ±sal bir vektÃ¶r ata. Bu vektÃ¶rler sayesinde:
# Kelimeler arasÄ±ndaki anlamsal yakÄ±nlÄ±k Ã¶ÄŸreniliyor.
# AynÄ± baÄŸlam geÃ§en kelimeler, uzayda da birbirine yakÄ±n olur.
# Araba -> [0.21, -0.43, 0.92, ........, 0.01] 100 veya 300+ boyutlu.

# GÃ¼zel ek Ã¶zellik => VektÃ¶r cebiri bile yapÄ±labilir.
# vec("king") - vec("man") + vec("woman") = vec("queen")

# Nerede kullanÄ±lÄ±r? 

# Derin Ã¶ÄŸrenme.
# Chatbot, anlamsal arama

corpus = [
    "NLP Ã§ok eÄŸlenceli alan",
    "DoÄŸal dil iÅŸleme Ã§ok Ã¶nemli",
    "EÄŸlenceli projeler yapÄ±yoruz"
]
import gensim
from gensim.models import Word2Vec
from nltk.tokenize import word_tokenize

#her cÃ¼mleyi tokenize et. kelime listesi oluÅŸtur.
# kelimeleri parÃ§ala, liste haline getir.
tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in corpus]
print("******")
print(tokenized_sentences)

model = Word2Vec(tokenized_sentences, vector_size=100, window=5, min_count=1, workers=2)
"""vector_size=100: Her kelime 100 boyutlu bir vektÃ¶rle temsil edilir.

window=5: Model, her kelimenin Ã§evresindeki 5 kelimeyi dikkate alarak baÄŸlam Ã¶ÄŸrenir.

min_count=1: En az 1 kez geÃ§en kelimeler modele dahil edilir.

workers=2: 2 CPU Ã§ekirdeÄŸi kullanÄ±larak model paralel olarak eÄŸitilir (hÄ±zlÄ± eÄŸitim).

ğŸ“Œ Bu satÄ±r modelin kelime vektÃ¶rlerini Ã¶ÄŸrenmesini saÄŸlar."""
print("*******")
print(model.wv['nlp'])
print("*******")
print(model.wv.most_similar('nlp'))
""" | Terim                     | AnlamÄ±                                                  |
| ------------------------- | ------------------------------------------------------- |
| `model`                   -> Word2Vec eÄŸitimi yapÄ±lan modelin tamamÄ±                 |
| `model.wv`                -> Sadece kelime vektÃ¶rlerine eriÅŸim iÃ§in kullanÄ±lan parÃ§a |
| `model.wv['kelime']`      -> O kelimenin sayÄ±sal temsili (embedding)                 |
| `model.wv.most_similar()` -> En benzer kelimeleri bulur                              |"""
#Word2Vec, kelimeleri sayÄ±lara (vektÃ¶rlere) dÃ¶nÃ¼ÅŸtÃ¼ren bir yapay zeka modelidir. Ama rastgele deÄŸil â€” bu sayÄ±lar, kelimelerin anlamlarÄ±nÄ± yansÄ±tÄ±r!



# Sentence Embedding


corpus = [
    "NLP Ã§ok eÄŸlenceli alan",
    "NLP Ã§ok Ã¶nemli",
    "EÄŸlenceli projeler yapÄ±yoruz"
]
tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in corpus]

model = Word2Vec(tokenized_sentences, vector_size=100, window=5, min_count=1, workers=2)

# Ortalama VektÃ¶r Alma
import numpy as np

def sentence_vector(sentence):
    words = word_tokenize(sentence.lower())
    vectors = []
    for word in words:
        if word in model.wv:
            vectors.append(model.wv[word])
    if len(vectors) > 0:
        return np.mean(vectors, axis=0)
    return np.zeros(100)

vec1 = sentence_vector(corpus[0]) #corpustaki ilk cÃ¼mlesinin 100 boyutlu ortalama vektÃ¶rÃ¼.
vec2 = sentence_vector(corpus[1]) #corpustaki ikinci cÃ¼mlesinin 100 boyutlu ortalama vektÃ¶rÃ¼.
"""******************************************************
Ã‡Ä±ktÄ±lar -0.1 ile +0.1 civarÄ±nda ondalÄ±klÄ± 100 sayÄ±lÄ±k diziler gibi gÃ¶rÃ¼nÃ¼r.
Bu vektÃ¶rler artÄ±k:
Ä°ki cÃ¼mle arasÄ±ndaki benzerliÄŸi (Ã¶r. kosinÃ¼s benzerliÄŸi) hesaplamak,
KÃ¼meleme / sÄ±nÄ±flandÄ±rma yapmak,
GÃ¶rselleÅŸtirme (t-SNE, PCA) oluÅŸturmak
gibi iÅŸlemlerde kullanÄ±labilir.
***************************************************"""
print(vec1)
print(vec2)

def cosine_similarity(vec1, vec2):
    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))

# cosinesimilarty(a,b) = a.b / |a| * |b| => -1,1 arasÄ±nda deÄŸer dÃ¶ner. nor
#Bu, vec1 ile vec2 arasÄ±ndaki aÃ§Ä±sal benzerliÄŸi hesaplar. cosÃ¼ verir iki vektÃ¶rÃ¼n aralarÄ±ndaki aÃ§Ä± yani
#np.dot(vec1, vec2)
#Ä°ki vektÃ¶rÃ¼n iÃ§ Ã§arpÄ±mÄ± (dot product) alÄ±nÄ±r.
"""
Ä°ki vektÃ¶r paralel ve aynÄ± yÃ¶ndeyse â†’ 1 anlamsal benzerlik var
Ters yÃ¶ndeyse â†’ -1 anlam tam tersi
Birbirine dikse â†’ 0 anlamsal benzerlik yok
deÄŸerini dÃ¶ndÃ¼rÃ¼r.  cÃ¼mle benzerliÄŸini Ã§ok kullanÄ±rÄ±z
"""

# Average Word Embedding

print(cosine_similarity(vec1, vec2))

# Sentence-BERT (SBERT) 
# Bu dosya Ã¼zerinde uygulayalÄ±m.
from sentence_transformers import SentenceTransformer

model = SentenceTransformer("all-MiniLM-L6-v2")
#all-MiniLM-L6-v2: Hafif, hÄ±zlÄ± ve gÃ¼Ã§lÃ¼ bir SBERT modeli
cÃ¼mleler = [
    "NLP Ã§ok eÄŸlenceli alan",
    "NLP Ã§ok Ã¶nemli",
    "EÄŸlenceli projeler yapÄ±yoruz"
]

embeddings = model.encode(cÃ¼mleler, convert_to_tensor=False)

print(embeddings)
"""from sentence_transformers import SentenceTransformer, util

model = SentenceTransformer('all-MiniLM-L6-v2')  # Hafif, hÄ±zlÄ± ve gÃ¼Ã§lÃ¼ bir SBERT modeli

# CÃ¼mleleri tanÄ±mla
sentence1 = "NLP Ã§ok eÄŸlenceli alan"
sentence2 = "NLP Ã§ok Ã¶nemli"

# VektÃ¶rlere dÃ¶nÃ¼ÅŸtÃ¼r
embedding1 = model.encode(sentence1, convert_to_tensor=True)
embedding2 = model.encode(sentence2, convert_to_tensor=True)

 KosinÃ¼s benzerliÄŸi hesapla
similarity = util.pytorch_cos_sim(embedding1, embedding2)
print("Benzerlik:", similarity.item())"""
